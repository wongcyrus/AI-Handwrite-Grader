{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Marking Form\n",
    "1. Convert PDF into images.\n",
    "2. Highlight student's answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Linux tools and only required for the first run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get update\n",
    "!sudo apt-get -y install poppler-utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the student script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file = \"../data/TestScript.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extrat file name from pdf_file\n",
    "import os\n",
    "\n",
    "file_name = os.path.basename(pdf_file)\n",
    "file_name = os.path.splitext(file_name)[0]\n",
    "base_path = \"../marking_form/\" + file_name\n",
    "base_path_images = base_path + \"/images/\"\n",
    "base_path_annotations = base_path+\"/annotations/\"\n",
    "# create directory tree for base_path_images\n",
    "os.makedirs(base_path_images, exist_ok=True)\n",
    "os.makedirs(base_path_annotations, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert PDF to JPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read pdf and convert to images\n",
    "# https://stackoverflow.com/questions/46184239/how-to-convert-pdf-to-image-using-python\n",
    "from pdf2image import convert_from_path\n",
    "import os\n",
    "\n",
    "pages = convert_from_path(pdf_file, fmt='jpeg')\n",
    "# extrat file name from pdf_file\n",
    "file_name = os.path.basename(pdf_file)\n",
    "file_name = os.path.splitext(file_name)[0]\n",
    "\n",
    "for count, page in enumerate(pages):\n",
    "    page.save(f'{base_path_images}{count}.jpg', 'JPEG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "\n",
    "def update_json_file(annotations, path):\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(annotations, f, indent=4)   \n",
    "\n",
    "def image_to_data_url(filename):\n",
    "    ext = filename.split(\".\")[-1]\n",
    "    prefix = f\"data:image/{ext};base64,\"\n",
    "    with open(filename, \"rb\") as f:\n",
    "        img = f.read()\n",
    "    return prefix + base64.b64encode(img).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = 'cyrus-testing-2023'\n",
    "!gcloud config set project {project_id}\n",
    "!gcloud auth application-default set-quota-project {project_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(\n",
    "    # your Google Cloud Project ID or number\n",
    "    # environment default used is not set\n",
    "    project=project_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, Part\n",
    "import vertexai.preview.generative_models as generative_models\n",
    "\n",
    "vertexai.init(project=project_id, location=\"us-central1\")\n",
    "model = GenerativeModel(\"gemini-1.5-pro-002\") \n",
    "\n",
    "def ocr(prompt:str, filePath:str):       \n",
    "    with open(filePath, \"rb\") as f:\n",
    "        data = f.read()\n",
    "    image1 = Part.from_data(mime_type=\"image/png\", data=data)\n",
    "    generation_config = {\n",
    "        \"max_output_tokens\": 8192,\n",
    "        \"temperature\": 0,\n",
    "        \"top_p\": 0.5,\n",
    "    }\n",
    "    safety_settings = {\n",
    "        generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "        generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "        generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "        generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "    }\n",
    "    responses = model.generate_content(\n",
    "        [image1, prompt],\n",
    "        generation_config=generation_config,\n",
    "        safety_settings=safety_settings,\n",
    "        stream=True,\n",
    "    )\n",
    "    text = \"\"\n",
    "    for response in responses:        \n",
    "        text += response.text\n",
    "    print(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "prompt = \"\"\"Extract the coordinate of restangle cells bound box from the image.\n",
    "Label is the question number such as Q1, Q2, Q3, etc. and it is the text inside the top right of the cell.\n",
    "Cells are not in the form of table, they are just rectangle boxes.\n",
    "Bound box is no overlap with other bound box.\n",
    "Output should be in the following JSON Array format:\n",
    "[\n",
    "    {\n",
    "        \"x\": 152,\n",
    "        \"y\": 313,\n",
    "        \"width\": 702,\n",
    "        \"height\": 243,\n",
    "        \"label\": \"1\"\n",
    "    },\n",
    "    {\n",
    "        \"x\": 152,\n",
    "        \"y\": 313,\n",
    "        \"width\": 702,\n",
    "        \"height\": 243,\n",
    "        \"label\": \"2\"\n",
    "    },\n",
    "    .....\n",
    "]\n",
    "Do not include any explanations, only provide a RFC8259 compliant JSON response following this format without deviation in the following format.\n",
    "Output:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "aiAnnoation = {}\n",
    "number_of_pages = 1\n",
    "for i in range(number_of_pages):\n",
    "    image_path = base_path_images + f\"{i}.jpg\"\n",
    "    result = ocr(prompt, image_path) \n",
    "    result = result.strip(\"```json\\n\").strip(\"```\")\n",
    "    result = json.loads(result)\n",
    "    aiAnnoation[str(i)] = result\n",
    "\n",
    "print(aiAnnoation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "backup = copy.deepcopy(aiAnnoation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Open an image file\n",
    "with Image.open(image_path) as img:\n",
    "    # Get width and height\n",
    "    width, height = img.size\n",
    "\n",
    "print(f\"Width: {width}, Height: {height}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import copy\n",
    "aiAnnoation = copy.deepcopy(backup)\n",
    "\n",
    "x_scale = width / 1000.0\n",
    "y_scale = height / 1000.0\n",
    "# x_scale = 1\n",
    "# y_scale = 1\n",
    "for i in range(number_of_pages):\n",
    "    for item in aiAnnoation[str(i)]:\n",
    "        item['x'] = int(round(item['x'] * x_scale))\n",
    "        item['y'] = int(round(item['y'] * y_scale)) \n",
    "        item['width'] = int(round(item['width'] * x_scale))\n",
    "        item['height'] = int(round(item['height'] * y_scale))\n",
    "\n",
    "\n",
    "ai_annotations_path = base_path_annotations + \"ai_annotations.json\"\n",
    "\n",
    "# Save the aiAnnoation variable to a JSON file\n",
    "with open(ai_annotations_path, \"w\") as f:\n",
    "    json.dump(aiAnnoation, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jupyter_bbox_widget import BBoxWidget\n",
    "import ipywidgets as widgets\n",
    "import json\n",
    "import glob\n",
    "\n",
    "page = 1\n",
    "pageAndBoxingBoxes={}\n",
    "\n",
    "files = sorted(glob.glob(base_path_images + \"*.jpg\"))\n",
    "\n",
    "w_progress = widgets.IntProgress(value=0, max=len(files), description=\"Progress\")\n",
    "annotations_path = base_path_annotations + \"annotations.json\"\n",
    "ai_annotations_path = base_path_annotations + \"ai_annotations.json\"\n",
    "\n",
    "annotations = {}\n",
    "# if annotations_path exists, load annotations from it\n",
    "if os.path.exists(ai_annotations_path):\n",
    "    with open(ai_annotations_path, \"r\") as f: \n",
    "        annotations = json.load(f) \n",
    "\n",
    "if os.path.exists(annotations_path):\n",
    "    with open(annotations_path, \"r\") as f: \n",
    "        annotations = json.load(f) \n",
    "\n",
    "question_widget = widgets.Text(value=\"\", placeholder=\"\", description=\"Question:\")\n",
    "\n",
    "w_bbox = BBoxWidget(\n",
    "    image=image_to_data_url(files[0])   \n",
    ")\n",
    "w_bbox.attach(question_widget, name=\"label\")\n",
    "w_bbox.bboxes = annotations[str(w_progress.value)] if str(w_progress.value) in annotations else []\n",
    "\n",
    "# when Skip button is pressed we move on to the next file\n",
    "def on_skip():\n",
    "    w_progress.value += 1\n",
    "    # open new image in the widget\n",
    "    image_file = files[w_progress.value]\n",
    "    w_bbox.image = image_to_data_url(image_file)     \n",
    "    w_bbox.bboxes = annotations[str(w_progress.value)] if str(w_progress.value) in annotations else []\n",
    "\n",
    "\n",
    "w_bbox.on_skip(on_skip)\n",
    "\n",
    "# when Submit button is pressed we save current annotations\n",
    "# and then move on to the next file\n",
    "def on_submit():\n",
    "    image_file = files[w_progress.value]\n",
    "    # save annotations for current image\n",
    "    annotations[str(w_progress.value)] = w_bbox.bboxes\n",
    "    update_json_file(annotations, annotations_path)\n",
    "    # move on to the next file\n",
    "    on_skip()\n",
    "\n",
    "\n",
    "w_bbox.on_submit(on_submit)\n",
    "w_out = widgets.Output()\n",
    "\n",
    "def on_bbox_change(change):\n",
    "    w_out.clear_output(wait=True)\n",
    "    with w_out:\n",
    "        print(json.dumps(change[\"new\"], indent=4))\n",
    "        pageAndBoxingBoxes[w_progress.value] = change[\"new\"]\n",
    "\n",
    "\n",
    "w_bbox.observe(on_bbox_change, names=[\"bboxes\"])\n",
    "\n",
    "w_container = widgets.VBox(\n",
    "    [\n",
    "        widgets.HBox(\n",
    "            [\n",
    "                question_widget            \n",
    "            ]\n",
    "        ),\n",
    "        w_progress,\n",
    "        w_bbox,\n",
    "        w_out,\n",
    "    ]\n",
    ")\n",
    "w_container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
